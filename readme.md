# Отчет по проекту “Частеречная разметка (POS-tagging) русской региональной ассоциативной базы данных”

## Introduction

В области обработки естественного языка существуют задачи, связанные с разделением слов на категории, которые определяются морфологическими, синтаксическими и семантическими признаками конкретного слова, например, на части речи [1]. Например, находиться, усесться, начинать - это глаголы; ацтек, клуб, смерть - существительные, и т.д.

Целью данного проекта является выполнение автоматической разметки данных по частям речи (POS-tagging, part-of-speech tagging) при помощи различных библиотек для обработки русскоязычных текстов. После этого необходимо сравнить методы и выбрать наилучшее решение.

## Data

Для работы была взята русская региональная ассоциативная база данных (СИБАС) [2]. Специфичность данных заключается в том, что данные представлены в виде пар слов: респондентам показывали слова-стимулы (которые были составлены психо-лингвистами)  (всего 1000 различных слов), и они должны были ответить словом-реакцией, которое ассоциируется у них со словом-стимулом. То есть у слов нет контекста в стандартном понимании (оно не стоит в предложении, есть лишь пара: стимул-реакция).

Ниже приведен пример данных: 

| **Стимул** | **Реакция** |
|------------|-------------|
| абсолютный | агент       |
| бабушка    | носки       |
| слышать    | наушники    |

## Methods

Все скрипты доступны на гитхабе:  [https://github.com/TohaRhymes/pos-tagging-practise](https://github.com/TohaRhymes/pos-tagging-practise)

Для реализации проекта были использованы различные библиотеки для обработки текстов на русском языке с возможностью разметки слов по частям речи. Есть два вида методов:
- __Работающие на основе словарей__ (реализованы, например, в библиотеках pymorphy2, nltk, natasha и др.).
- __Работающие на нейронных сетях__ (реализованы, например, в библиотеках rnnmorph, deeppavlov и др.). Хотя в каких-то сложных задачах они показывают невероятный перфоманс, может оказаться, что простые и надежные модели работают лучше BERT’ов и трансформеров. Поэтому сразу брать и  использовать нейронные сети не имеет смысла. 

## Результаты so far

Везде, где не указано иначе - уже результаты получены. 

0) Предобработка. Была сделана предобработка данных, убраны фразы (реакции, состоящие из более чем одного слова - содержащие пробел), а также слова, содержащие спец символы. В результате из 167889 пар осталось 140269 пар слов.
1) Использование библиотек для определения частей речи.
    - __Библиотека pymorphy2 [3]__. Самый простой из возможных способов решения проблемы, работающий на словарях: принимает слово, возвращает его возможные части речи (например, у слова “мой” часть речи может быть как глагол, так и местоимение). Мы сохраняем все варианты.
    - __Библиотека natasha [4]__. Использует pymorphy2 (то есть словари), а так же дополнительные правила взаимодействия слов, частей речи – морфологические и синтаксические разборы текста. В результате уже возвращает одно значение, в отличие от pymorphy2. 
2) Так как уже раньше говорилось, что у нас нет контекста в обычном смысле этого слова, поэтому нет гарантии, что такие, казалось бы, простые методы будут работать хуже нейросетей. Однако, для полноты картины, кроме стандартных методов был рассмотрен ряд библиотек, работающих на нейронных сетях:
    - __Библиотека deeppavlov [5]__. В данный момент проходит работа с этой библиотекой. 
        - Сначала там возникало много проблем с установкой (даже на виртуальное окружение Python). 
        - Для русского языка библиотека предлагает два варианта моделей: основанная на BERT (англ. Bidirectional Encoder Representations from Transformers – подробнее можно почитать в гугле) и двунаправленная LSTM на основе символов. По данным документации, эффективность BERT 97,8% против 96,2% у LSTM.
    - __Библиотека rnnmorph [6]__. Решение на рекуррентных нейросетях (RNN). Еще не запускал.
    - __Лучшее решение с Диалога GramEval2020 [7]__. Решение на двунаправленной LSTM (англ. Long-Short Term Memory – подробнее можно почитать в гугле). Еще не запускал.
3)  После получения тегов каждой из библиотек, был произведен маппинг, используя данные по граммемам (универсальным обозначениям частей речи) [8].

## Future plans
1) установить оставшиеся 2 библиотеки, проверить
2) запустить оставшиеся NLP DL фреймворки.
3) взять тестовые данные, прогнать все на полученном пайплайне, и сравнить результаты

## Discussion
Практический смысл данной работы заключается в попытке определить наилучший метод (из существующих) для быстрой автоматической разметки похожих ассоциативных баз. Размеченные данные могут использоваться лингвистами уже для дальнейшего анализа языка.

# References
[1] В. М. Алпатов. О разных подходах к выделению частей речи // Вопросы языкознания. — 1986. — Вып. 4. — С. 37–46.

[2] Сибирский ассоциативный словарь русского языка (СИБАС). http://adictru.nsu.ru/

[3] Korobov M.: Morphological Analyzer and Generator for Russian and
Ukrainian Languages // Analysis of Images, Social Networks and Texts,
pp 320-332 (2015).

[4] Natasha project. https://github.com/natasha/natasha

[5] Burtsev M. et al. DeepPavlov: Open-Source Library for Dialogue Systems //Proceedings of ACL 2018, System Demonstrations. – 2018. – С. 122-127.

[6] Ilya Gusev. Morphological analyzer for russian and english languages based on neural networks and dictionary-lookup systems.  https://github.com/IlyaGusev/rnnmorph

[7] Anastasyev D. G. et al. Part-of-speech Tagging with
Rich Language Description //Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference “Dialogue 2017” - Moscow, May 31—June 3, 2017

[8] http://opencorpora.org/dict.php?act=gram











